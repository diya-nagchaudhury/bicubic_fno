{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 \n",
    "import scipy.io as sio\n",
    "import scipy.ndimage\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/home/diya/Projects/flow_super_resolution/dataset/hit_data/flow_sr_ns2d/data_HR/matlab_data/' \n",
    "output_path = '/home/diya/Projects/flow_super_resolution/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load high-resolution data generated from MATLAB code\n",
    "hr_files = sorted(glob.glob(input_path))\n",
    "\n",
    "high_res_data = []\n",
    "for file in hr_files:\n",
    "    mat_data = sio.loadmat(file)\n",
    "    # Extract vorticity field 'omg' as mentioned in the paper\n",
    "    high_res_data.append(mat_data['omg'])\n",
    "    \n",
    "high_res_data = np.array(high_res_data)  # Shape: [n_samples, 128, 128]\n",
    "print(high_res_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_downsample(data, target_size=(16, 16)):\n",
    "    n_samples = data.shape[0]\n",
    "    low_res_data = np.zeros((n_samples, target_size[0], target_size[1]))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Reshape to perform average pooling\n",
    "        h, w = data[i].shape\n",
    "        pool_size = (h // target_size[0], w // target_size[1])\n",
    "        reshaped = data[i].reshape(target_size[0], pool_size[0], \n",
    "                                  target_size[1], pool_size[1])\n",
    "        low_res_data[i] = reshaped.mean(axis=(1, 3))\n",
    "    \n",
    "    return low_res_data\n",
    "\n",
    "# Generate low-resolution data\n",
    "low_res_data_16x16 = average_downsample(high_res_data, target_size=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 204, Validation: 26, Test: 26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train (80%) and Temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    low_res_data_16x16, high_res_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Validation (10%) and Test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciton for bicubic upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubic_upsample(data, target_size=(128, 128)):\n",
    "    upsampled_data = np.zeros((data.shape[0], target_size[0], target_size[1]))\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        upsampled_data[i] = scipy.ndimage.zoom(data[i], \n",
    "                                               (target_size[0]/data.shape[1], target_size[1]/data.shape[2]), \n",
    "                                               order=3)  # Bicubic interpolation\n",
    "    \n",
    "    return upsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply bicubic upsampling to the LR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bicubic Upsampled Training Data: (204, 128, 128)\n",
      "Bicubic Upsampled Validation Data: (26, 128, 128)\n",
      "Bicubic Upsampled Test Data: (26, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Upsample from 8x8 to 128x128 using bicubic interpolation\n",
    "upsampled_train_16x16 = bicubic_upsample(X_train, target_size=(128, 128))\n",
    "upsampled_val_16x16 = bicubic_upsample(X_val, target_size=(128, 128))\n",
    "upsampled_test_16x16 = bicubic_upsample(X_test, target_size = (128, 128))\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Bicubic Upsampled Training Data: {upsampled_train_16x16.shape}\")\n",
    "print(f\"Bicubic Upsampled Validation Data: {upsampled_val_16x16.shape}\")\n",
    "print(f\"Bicubic Upsampled Test Data: {upsampled_test_16x16.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the upsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved upsampled datasets in /home/diya/Projects/flow_super_resolution/outputs/bicubic_inter_results/\n"
     ]
    }
   ],
   "source": [
    "# Define save paths\n",
    "bicubic_save_dir = output_path\n",
    "os.makedirs(bicubic_save_dir, exist_ok=True)\n",
    "\n",
    "# Save the data\n",
    "np.save(bicubic_save_dir + 'train.npy', upsampled_train_16x16)\n",
    "np.save(bicubic_save_dir + 'val.npy', upsampled_val_16x16)\n",
    "\n",
    "print(f\"Saved upsampled datasets in {bicubic_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the upsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_metrics(prediction, target):\n",
    "    mse = np.mean((prediction - target) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    target_range = np.max(target) - np.min(target)\n",
    "    nrmse = rmse / target_range if target_range != 0 else rmse\n",
    "    data_range = target_range\n",
    "    psnr = 20 * np.log10(data_range / rmse) if rmse > 0 else float('inf')\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"nrmse\": nrmse, \"psnr\": psnr}\n",
    "\n",
    "def visualize_results(low_res_samples, bilinear_samples, bicubic_samples, \n",
    "                                fno_outputs, high_res_samples, num_samples=3, dataset_type=\"Test\", save_dir=None, save=True):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    indices = np.random.choice(len(low_res_samples), num_samples, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "        fig.suptitle(f\"{dataset_type} Sample {i+1} - Super-Resolution Comparison\", fontsize=18)\n",
    "\n",
    "        titles = [\n",
    "            r\"$u(\\Omega_L, t)$\", \n",
    "            r\"$BL(u(\\Omega_L, t))$\", \n",
    "            r\"$BC(u(\\Omega_L, t))$\", \n",
    "            r\"$u(\\Omega_H, t)$\", \n",
    "            r\"$f_\\theta(u(\\Omega_L, t))$\"\n",
    "        ]\n",
    "\n",
    "        images = [\n",
    "            low_res_samples[idx], \n",
    "            bilinear_samples[idx], \n",
    "            bicubic_samples[idx],\n",
    "            high_res_samples[idx], \n",
    "            fno_outputs[idx]\n",
    "        ]\n",
    "\n",
    "        for ax, title, img in zip(axes.flat[:5], titles, images):\n",
    "            im = ax.imshow(img, cmap='coolwarm')\n",
    "            ax.set_title(title)\n",
    "            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        # Energy Spectrum (dummy example)\n",
    "        ax_spec = axes[1, 2]\n",
    "        k = np.arange(1, 100)\n",
    "        ax_spec.loglog(k, 1/k, label=r\"$u(\\Omega_H, t)$\")\n",
    "        ax_spec.loglog(k, 1/(k**1.2), label=r\"$f_\\theta(u(\\Omega_L, t))$\")\n",
    "        ax_spec.loglog(k, 1/(k**1.4), label=r\"$BC(u(\\Omega_L, t))$\")\n",
    "        ax_spec.loglog(k, 1/(k**1.6), label=r\"$BL(u(\\Omega_L, t))$\")\n",
    "        ax_spec.set_title(\"Energy Spectrum\")\n",
    "        ax_spec.set_xlabel(r\"$|\\mathbf{k}|$\")\n",
    "        ax_spec.set_ylabel(r\"$E(\\mathbf{k})$\")\n",
    "        ax_spec.legend()\n",
    "        ax_spec.grid(True, which=\"both\", ls=\"--\", lw=0.5)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(save_dir, f\"{dataset_type.lower()}_composite_sample_{i+1}.png\"), dpi=150)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    print(f\"Saved {num_samples} composite visualizations to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def calculate_metrics(prediction, target):\n",
    "    mse = np.mean((prediction - target) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    target_range = np.max(target) - np.min(target)\n",
    "    nrmse = rmse / target_range if target_range != 0 else rmse\n",
    "    psnr_value = 20 * np.log10(target_range / rmse) if rmse > 0 else float('inf')\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"nrmse\": nrmse, \"psnr\": psnr_value}\n",
    "\n",
    "\n",
    "def visualize_and_save_samples(\n",
    "    low_res_samples,\n",
    "    high_res_samples,\n",
    "    upsampled_samples,\n",
    "    num_samples=5,\n",
    "    dataset_type=\"Training\",\n",
    "    save_dir=None\n",
    "):\n",
    "    if save_dir is None:\n",
    "        raise ValueError(\"save_dir must be specified.\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # LaTeX + serif font styling\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "        \"axes.titlesize\": 11,\n",
    "        \"axes.labelsize\": 11,\n",
    "        \"xtick.labelsize\": 11,\n",
    "        \"ytick.labelsize\": 11\n",
    "    })\n",
    "\n",
    "    x = y = np.linspace(0, 1, 128)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    indices = np.random.choice(len(low_res_samples), num_samples, replace=False)\n",
    "    all_metrics = []\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        lr_img = low_res_samples[idx]\n",
    "        hr_img = high_res_samples[idx]\n",
    "        pred_img = upsampled_samples[idx]\n",
    "        error_img = np.abs(pred_img - hr_img)\n",
    "        metrics = calculate_metrics(pred_img, hr_img)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(6, 6), constrained_layout=True)\n",
    "        images = [\n",
    "            (np.kron(lr_img, np.ones((8, 8))), r'$(a)$ Low Resolution'),\n",
    "            (hr_img, r'$(b)$ Ground Truth'),\n",
    "            (pred_img, r'$(c)$ Prediction'),\n",
    "            (error_img, r'$(d)$ Error Map')\n",
    "        ]\n",
    "\n",
    "        for j, (ax, (img, title)) in enumerate(zip(axes.flat, images)):\n",
    "            ax.contourf(X, Y, img, levels=50, cmap=cm.RdBu_r)\n",
    "            ax.set_title(title, pad=6)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks([0, 0.5, 1])\n",
    "            ax.set_yticks([0, 0.5, 1])\n",
    "            ax.set_xticklabels([r'$0$', r'$0.5$', r'$1$'])\n",
    "            ax.set_yticklabels([r'$0$', r'$0.5$', r'$1$'])\n",
    "\n",
    "            # Add x/y labels only on the appropriate sides\n",
    "            if j in [2, 3]:\n",
    "                ax.set_xlabel(r'$x$')\n",
    "            if j in [0, 2]:\n",
    "                ax.set_ylabel(r'$y$')\n",
    "\n",
    "        fig.suptitle(\n",
    "            rf\"{i+1} -- RMSE: {metrics['rmse']:.4f}, \"\n",
    "            rf\"PSNR: {metrics['psnr']:.2f} dB\",\n",
    "            fontsize=12\n",
    "        )\n",
    "\n",
    "        plt.savefig(os.path.join(save_dir, f\"{dataset_type.lower()}_sample_{i+1}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Save Average Metrics\n",
    "    avg_metrics = {\n",
    "        \"mse\": np.mean([m[\"mse\"] for m in all_metrics]),\n",
    "        \"rmse\": np.mean([m[\"rmse\"] for m in all_metrics]),\n",
    "        \"nrmse\": np.mean([m[\"nrmse\"] for m in all_metrics]),\n",
    "        \"psnr\": np.mean([m[\"psnr\"] for m in all_metrics])\n",
    "    }\n",
    "\n",
    "    csv_path = os.path.join(save_dir, \"average_metrics.csv\")\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"dataset_type\", \"mse\", \"rmse\", \"nrmse\", \"psnr\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\"dataset_type\": dataset_type, **avg_metrics})\n",
    "\n",
    "    print(f\"\\n{dataset_type} Set Average Metrics:\")\n",
    "    print(f\"MSE: {avg_metrics['mse']:.4f}\")\n",
    "    print(f\"RMSE: {avg_metrics['rmse']:.4f}\")\n",
    "    print(f\"NRMSE: {avg_metrics['nrmse']:.4f}\")\n",
    "    print(f\"PSNR: {avg_metrics['psnr']:.2f} dB\")\n",
    "    print(f\"Visualization completed for {num_samples} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bicubic upsampled results\n",
    "save_dir = output_path\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Average Metrics:\n",
      "MSE: 103.4405\n",
      "RMSE: 10.0246\n",
      "NRMSE: 0.0652\n",
      "PSNR: 23.75 dB\n",
      "Visualization completed for 204 samples.\n",
      "\n",
      "Validation Set Average Metrics:\n",
      "MSE: 103.8150\n",
      "RMSE: 10.0334\n",
      "NRMSE: 0.0650\n",
      "PSNR: 23.77 dB\n",
      "Visualization completed for 26 samples.\n",
      "\n",
      "Test Set Average Metrics:\n",
      "MSE: 98.4354\n",
      "RMSE: 9.7432\n",
      "NRMSE: 0.0639\n",
      "PSNR: 23.93 dB\n",
      "Visualization completed for 26 samples.\n"
     ]
    }
   ],
   "source": [
    "visualize_and_save_samples(X_train, y_train, upsampled_samples=upsampled_train_16x16, num_samples=len(X_train), dataset_type=\"Training\", save_dir=save_dir+'bicubic_train')\n",
    "visualize_and_save_samples(X_val, y_val, upsampled_samples=upsampled_val_16x16, num_samples=len(X_val), dataset_type=\"Validation\", save_dir=save_dir+'bicubic_val')\n",
    "visualize_and_save_samples(X_test, y_test, upsampled_samples=upsampled_test_16x16, num_samples=len(X_test), dataset_type=\"Test\", save_dir=save_dir+'bicubic_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhu_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
